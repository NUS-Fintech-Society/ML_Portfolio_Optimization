{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM_Final.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"dI7tC6umUvAJ"},"source":["Loading Libraries\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"sdYgDg9aqDOM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639334790275,"user_tz":-480,"elapsed":6603,"user":{"displayName":"mayve","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10280033240280404932"}},"outputId":"1ffa7ac8-f424-4ef8-97e3-a3fed718ee32"},"source":["pip install yfinance"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting yfinance\n","  Downloading yfinance-0.1.67-py2.py3-none-any.whl (25 kB)\n","Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.19.5)\n","Collecting lxml>=4.5.1\n","  Downloading lxml-4.6.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.3 MB)\n","\u001b[K     |████████████████████████████████| 6.3 MB 8.0 MB/s \n","\u001b[?25hRequirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.10)\n","Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.23.0)\n","Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.1.5)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24->yfinance) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2.10)\n","Installing collected packages: lxml, yfinance\n","  Attempting uninstall: lxml\n","    Found existing installation: lxml 4.2.6\n","    Uninstalling lxml-4.2.6:\n","      Successfully uninstalled lxml-4.2.6\n","Successfully installed lxml-4.6.4 yfinance-0.1.67\n"]}]},{"cell_type":"code","metadata":{"id":"GpCXa48MXpU1"},"source":["#load libraries \n","# pip install yfinance\n","import pandas as pd\n","import math\n","import pandas_datareader as web\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","import keras\n","import tensorflow as tf\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, LSTM, Input, Activation,concatenate\n","from keras import optimizers\n","from keras.preprocessing.sequence import TimeseriesGenerator\n","from keras.layers.advanced_activations import LeakyReLU\n","import matplotlib.pyplot as plt\n","plt.style.use('fivethirtyeight')\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from numpy.random import seed\n","from scipy.stats import pearsonr"],"metadata":{"id":"Gy_ym3IBYk0H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install -q -U keras-tuner"],"metadata":{"id":"iDNXaOzTcaJj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639334826208,"user_tz":-480,"elapsed":4016,"user":{"displayName":"mayve","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10280033240280404932"}},"outputId":"3718bcec-7294-458a-9eee-00d0ce8a9ea6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 27.8 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 20 kB 28.6 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 30 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 40 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 61 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 81 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 98 kB 2.9 MB/s \n","\u001b[?25h"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","import keras_tuner as kt"],"metadata":{"id":"1gcb4buVcZ0_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Loading Dataset"],"metadata":{"id":"negCBO1KVVyw"}},{"cell_type":"code","metadata":{"id":"PpX70qxQYD08","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639334848015,"user_tz":-480,"elapsed":17485,"user":{"displayName":"mayve","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10280033240280404932"}},"outputId":"6f1af922-ee61-4135-f36c-84369213f5b0"},"source":["#load dataset to calculate RSI\n","from google.colab import drive \n","drive.mount('/content/gdrive') "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["df = pd.read_csv('/content/gdrive/MyDrive/Week 4 Discovery/ohlc_data.csv')"],"metadata":{"id":"QginfYKl9p9D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tickers = df['ticker'].unique()"],"metadata":{"id":"GfB-M5p_YAfI"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qHzeGMCfYL4-","executionInfo":{"status":"ok","timestamp":1639334866283,"user_tz":-480,"elapsed":10529,"user":{"displayName":"mayve","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10280033240280404932"}},"outputId":"4d928d9a-eb19-428f-a6c7-d355310d6c88"},"source":["#load log returns \n","import yfinance as yf\n","import numpy as np\n","from functools import reduce\n","#df= get_log_return(tickers, \"2001-01-02\", \"2020-12-31\")\n","def get_log_return(list_of_tickers, start_date, end_date, interval=\"1d\"): \n","    stocks = list()\n","    for ticker in list_of_tickers:\n","        ticker_data = yf.download(ticker, start=start_date, end=end_date, interval=interval)\n","        data={ticker: np.log(ticker_data['Adj Close']) - np.log(ticker_data['Adj Close'].shift(1))}\n","        #data = {ticker: ticker_data['Adj Close']}\n","        ticker_log_return = pd.DataFrame(data=data)\n","        #ticker_return = pd.DataFrame(data=data)\n","        stocks.append(ticker_log_return)\n","        #stocks.append(ticker_return)\n","    combined = reduce(lambda df1,df2: pd.merge(df1,df2,on='Date'), stocks)\n","    combined['date'] = ticker_log_return.index\n","    return combined\n","\n","df1= get_log_return(tickers, \"2001-01-02\", \"2020-12-31\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n","[*********************100%***********************]  1 of 1 completed\n"]}]},{"cell_type":"code","metadata":{"id":"Qk2CnkIbUz4A"},"source":[" def RSI(prices, n=14):\n","        deltas = np.diff(prices)\n","        seed = deltas[:n+1]\n","        up = seed[seed >= 0].sum()/n\n","        down = -seed[seed < 0].sum()/n\n","        rs = up/down\n","        rsi = np.zeros_like(prices)\n","        rsi[:n] = 100. - 100./(1.+rs)\n","\n","        for i in range(n, len(prices)):\n","            delta = deltas[i-1]  # The diff is 1 shorter\n","\n","            if delta > 0:\n","                upval = delta\n","                downval = 0.\n","            else:\n","                upval = 0.\n","                downval = -delta\n","\n","            up = (up*(n-1) + upval)/n\n","            down = (down*(n-1) + downval)/n\n","\n","            rs = up/down\n","            rsi[i] = 100. - 100./(1.+rs)\n","\n","        return rsi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Feature Selection"],"metadata":{"id":"1hSoHA69YSBJ"}},{"cell_type":"code","source":["#columns \n","df.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KJ0DSoQsYsZO","executionInfo":{"status":"ok","timestamp":1639334872414,"user_tz":-480,"elapsed":11,"user":{"displayName":"mayve","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10280033240280404932"}},"outputId":"1efaeba1-4f27-40a1-f9ae-66fc71b28d07"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume',\n","       'ticker'],\n","      dtype='object')"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["#check for correlation between input variables \n","seed(1)\n","pearsonr(df['Open'], df['High']) # 0.9998674775 strong positive correlation so used Open \n","pearsonr(df['Open'], df['Low']) #0.9998443635796925 strong positive correlation so used Open\n","pearsonr(df['Open'], df['Close']) #0.9997452726273854 strong positive correlation so used Close\n","pearsonr(df['Adj Close'], df['Close'])  # 0.986858324615821 trong positive correlation so used Adj Close\n","pearsonr(df['Adj Close'], df['Volume']) #-0.1832547243718910 weak correlation so will include Volume\n","\n","#since Adj Close price is used to calculate Log returns which will be the input variable, we will use Adj Close, RSI and Volumne as our final input variables"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w3C3LUVxYUf0","executionInfo":{"status":"ok","timestamp":1639301881578,"user_tz":-480,"elapsed":424,"user":{"displayName":"mayve","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10280033240280404932"}},"outputId":"115bb072-d405-49a9-f6c9-076e88af3ff2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(-0.18325472437189103, 0.0)"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["# Model Parameters and Sub Methods"],"metadata":{"id":"O5nVlunramJr"}},{"cell_type":"code","metadata":{"id":"U_Ilqpn7q2ss"},"source":["#set parameters for model\n","params = {\n","    \"BATCH_SIZE\": 50,\n","    \"EPOCHS\": 10,\n","    \"LR\": 0.00010000,\n","    \"TIME_STEPS\": 60\n","    }\n","\n","TIME_STEPS = params['TIME_STEPS']\n","BATCH_SIZE = params['BATCH_SIZE']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TJw-t1fsiSM5"},"source":["def build_timeseries(mat, y_col_index):\n","    \n","    dim_0 = mat.shape[0] - TIME_STEPS\n","    dim_1 = mat.shape[1]\n","\n","    x = np.zeros((dim_0, TIME_STEPS, dim_1))\n","    y = np.zeros((dim_0,))\n","\n","    print(\"Length of inputs\", dim_0)\n","\n","    for i in range(dim_0):\n","        x[i] = mat[i:TIME_STEPS+i]\n","        y[i] = mat[TIME_STEPS+i, y_col_index]\n","\n","    print(\"length of time-series - inputs\", x.shape)\n","    print(\"length of time-series - outputs\", y.shape)\n","\n","\n","    return x, y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"9NybSh7-cfgL"}},{"cell_type":"code","metadata":{"id":"z3pCe_GQrOOE"},"source":["def create_lstm_model(hp):\n","      \n","    lstm_model = Sequential()\n","    # Tune the number of units in the first Dense layer\n","    # Choose an optimal value between 32-512\n","    hp_units = hp.Int('units', min_value=50, max_value=100, step=TIME_STEPS)\n","    lstm_model.add(LSTM(units=hp_units, input_shape=(x_t.shape[1], x_t.shape[2]),\n","                       return_sequences=True,\n","                        kernel_initializer='random_uniform'))\n","    lstm_model.add(LSTM(60,activation='relu'))\n","    lstm_model.add(Dense(20,activation='relu'))\n","    lstm_model.add(Dropout(0.05))\n","    lstm_model.add(Dense(1, activation='sigmoid'))\n","        \n","    #compile the model\n","     # Tune the learning rate for the optimizer\n","  # Choose an optimal value from 0.01, 0.001, or 0.0001\n","    hp_learning_rate = hp.Choice('learning_rate', values= [ 0.01, 0.05, 0.1])\n","    #[0.01, 0.05, 0.1] \n","    lstm_model.compile(loss='mean_squared_error',optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate))\n","  \n","    return lstm_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ticker"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"Xb0d052dEBA2","executionInfo":{"status":"ok","timestamp":1639302831513,"user_tz":-480,"elapsed":400,"user":{"displayName":"mayve","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10280033240280404932"}},"outputId":"1775abc8-df59-4851-d5de-86449e7483be"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'AMGN'"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"8PYnybY0uAm-"},"source":["Loop Code"]},{"cell_type":"markdown","source":["Dates to run:\n","<br/>\n","1. 2001-2015 (train) --> 2016 (test)\n","<br/>\n","2. 2001-2016 (train) --> 2017 (test)\n","<br/>\n","3. 2001-2017 (train) --> 2018 (test)\n","<br/>\n","4. 2001-2018 (train) --> 2019 (test)\n","<br/>\n","5. 2001-2018 (train) --> 2020 (test)"],"metadata":{"id":"wBxnFjyme9fH"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R4spzjokty1f","executionInfo":{"status":"ok","timestamp":1639335828895,"user_tz":-480,"elapsed":147369,"user":{"displayName":"mayve","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10280033240280404932"}},"outputId":"89062602-1da4-452b-c221-fd81a0d079f8"},"source":["mse = {}\n","# total_predicted_returns= pd.DataFrame()\n","for ticker in tickers: #run for all tickers\n","  #Preparing Data \n","  df_ticker = df[df['ticker'] == ticker]\n","  df_ticker['RSI'] = RSI(df_ticker['Adj Close'])\n","  returns= df1[[ticker]].reset_index()\n","  returns['Date'] = returns['Date'].apply(lambda x : x.strftime('%Y-%m-%d'))\n","  df_ticker =   df_ticker[['RSI','Date','Volume']]\n","  returns.Date.astype('datetime64[ns]')\n","  df_ticker.Date.astype('datetime64[ns]')\n","  data = pd.merge(  df_ticker[['RSI','Date','Volume']],  returns , on='Date')[1:].rename(columns={ticker : 'Returns'})\n","  #Splitting Train Test Set\n","  data = data[data['Date'] <'2018-01-01']\n","  df_train = data[data['Date'] <'2017-01-01'].dropna()\n","  predicted_period = data[(data['Date'] >= '2017-01-01') & (data['Date'] < '2018-01-01')][['Date']]\n","  df_test =  data[len(data) - len(predicted_period) - 60:]\n","  train_cols = [\"RSI\",\"Returns\",\"Volume\"]\n","  x = df_train.loc[:,train_cols].values\n","  #Feature Scaling\n","  min_max_scaler = MinMaxScaler(feature_range = (0,1))\n","  x_train = min_max_scaler.fit_transform(x)\n","  x_test = min_max_scaler.transform(df_test.loc[:,train_cols])\n","  #build time_series based on 50 Time Step for training data \n","  x_t, y_t = build_timeseries(x_train, 1)\n","  print(\"Training size\", x_t.shape, y_t.shape)\n","\n","  #build time_series based on 50 Time Step for test data \n","  x_test_t, y_test_t = build_timeseries(x_test, 1)\n","  print(\"Test size\", x_test_t.shape, y_test_t.shape)\n","\n","  #validation set for keras tuner \n","  x_left,x_val = train_test_split(x_t, test_size=0.2, shuffle=False)\n","  y_left, y_val, =  train_test_split(y_t, test_size=0.2, shuffle=False)\n","\n","  #Keras Hyper Tuner\n","  tuner = kt.RandomSearch(\n","    create_lstm_model,\n","        objective='val_loss',\n","    max_trials=5)\n","  tuner.search(x_t, y_t, epochs=5, validation_data=(x_val,y_val))\n","  #tuner.search(x_t, y_t, epochs=5)\n","  #Best model summary \n","  lstm_model  = tuner.get_best_models()[0]   \n","  print(ticker,lstm_model.summary()) #Optimal parameters information \n","\n","  #Callback to prevent too much optimization \n","  callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n","\n","  #Model Training\n","  history_lstm = lstm_model.fit(x_t, y_t, epochs=params[\"EPOCHS\"], verbose=1, batch_size=BATCH_SIZE, callbacks=[callback],\n","                                shuffle=False)\n","  y_pred_lstm = lstm_model.predict(x_test_t, batch_size=BATCH_SIZE)\n","  y_pred_lstm = y_pred_lstm.flatten()\n","\n","  #Results\n","  error_lstm = mean_squared_error(y_test_t, y_pred_lstm)\n","  mse[ticker] = error_lstm\n","  print(ticker,error_lstm)\n","\n","  y_pred_lstm_org = (y_pred_lstm * min_max_scaler.data_range_[1]) + min_max_scaler.data_min_[1]   #Inverse Transform \n","  predicted_returns = pd.Series(y_pred_lstm_org).to_frame(ticker)\n","  predicted_period = predicted_period.reset_index().drop(columns=['index'])\n","  result = predicted_returns.join(predicted_period)\n","  if  total_predicted_returns.empty:\n","      total_predicted_returns= result \n","  else: \n","      total_predicted_returns = pd.merge(result, total_predicted_returns, on='Date')\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Length of inputs 3964\n","length of time-series - inputs (3964, 60, 3)\n","length of time-series - outputs (3964,)\n","Training size (3964, 60, 3) (3964,)\n","Length of inputs 251\n","length of time-series - inputs (251, 60, 3)\n","length of time-series - outputs (251,)\n","Test size (251, 60, 3) (251,)\n","INFO:tensorflow:Reloading Oracle from existing project ./untitled_project/oracle.json\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \n","/usr/local/lib/python3.7/dist-packages/sklearn/base.py:439: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n","  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","INFO:tensorflow:Reloading Tuner from ./untitled_project/tuner0.json\n","INFO:tensorflow:Oracle triggered exit\n","WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," lstm (LSTM)                 (None, 60, 50)            10800     \n","                                                                 \n"," lstm_1 (LSTM)               (None, 60)                26640     \n","                                                                 \n"," dense (Dense)               (None, 20)                1220      \n","                                                                 \n"," dropout (Dropout)           (None, 20)                0         \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 21        \n","                                                                 \n","=================================================================\n","Total params: 38,681\n","Trainable params: 38,681\n","Non-trainable params: 0\n","_________________________________________________________________\n","DIS None\n","Epoch 1/10\n","80/80 [==============================] - ETA: 0s - loss: 0.0034WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n","80/80 [==============================] - 17s 169ms/step - loss: 0.0034\n","Epoch 2/10\n","80/80 [==============================] - ETA: 0s - loss: 0.0028WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n","80/80 [==============================] - 14s 169ms/step - loss: 0.0028\n","Epoch 3/10\n","80/80 [==============================] - ETA: 0s - loss: 0.0028WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n","80/80 [==============================] - 13s 168ms/step - loss: 0.0028\n","Epoch 4/10\n","80/80 [==============================] - ETA: 0s - loss: 0.0028WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n","80/80 [==============================] - 14s 169ms/step - loss: 0.0028\n","Epoch 5/10\n","80/80 [==============================] - ETA: 0s - loss: 0.0028WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n","80/80 [==============================] - 13s 168ms/step - loss: 0.0028\n","Epoch 6/10\n","80/80 [==============================] - ETA: 0s - loss: 0.0028WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n","80/80 [==============================] - 13s 167ms/step - loss: 0.0028\n","Epoch 7/10\n","80/80 [==============================] - ETA: 0s - loss: 0.0028WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n","80/80 [==============================] - 14s 169ms/step - loss: 0.0028\n","Epoch 8/10\n","80/80 [==============================] - ETA: 0s - loss: 0.0028WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n","80/80 [==============================] - 14s 172ms/step - loss: 0.0028\n","Epoch 9/10\n","80/80 [==============================] - ETA: 0s - loss: 0.0028WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n","80/80 [==============================] - 13s 166ms/step - loss: 0.0028\n","Epoch 10/10\n","80/80 [==============================] - ETA: 0s - loss: 0.0028WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n","80/80 [==============================] - 13s 168ms/step - loss: 0.0028\n","DIS 0.0007380253229060349\n"]}]}]}